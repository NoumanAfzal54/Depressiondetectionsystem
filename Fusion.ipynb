{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "id": "GuTnaHZK_GnA",
    "outputId": "408335c0-0d99-40e1-da90-80f4756dd1d2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "* Running on public URL: https://6ac939a055b561fdcc.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://6ac939a055b561fdcc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n"
     ]
    }
   ],
   "source": [
    "# Depression Detection Audio-Video Fusion with Gradio GUI (Colab Ready)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import cv2\n",
    "import joblib\n",
    "import tempfile\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "import gradio as gr\n",
    "\n",
    "# Load models\n",
    "audio_model = tf.keras.models.load_model(\"C:\\\\Users\\\\hp\\\\Desktop\\\\New folder\\\\depression_model_finetuned.h5\")\n",
    "video_model = tf.keras.models.load_model(\"C:\\\\Users\\\\hp\\\\Desktop\\\\New folder\\\\densenet201_depression_model.keras\")\n",
    "scaler = joblib.load(\"C:\\\\Users\\\\hp\\\\Desktop\\\\New folder\\\\scaler.pkl\")\n",
    "\n",
    "IMG_HEIGHT, IMG_WIDTH = 224, 224\n",
    "FRAMES_PER_VIDEO = 20\n",
    "MAX_PAD_LEN = 216\n",
    "VIDEO_INFLUENCE_WEIGHT = 0.4\n",
    "\n",
    "def extract_frames(video_path, num_frames=FRAMES_PER_VIDEO):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    frames = []\n",
    "    for fid in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, fid)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            frames.append(np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8))\n",
    "            continue\n",
    "        frame = cv2.resize(frame, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    frames = np.array(frames).astype('float32') / 255.0\n",
    "    return frames\n",
    "\n",
    "def extract_audio_features(audio_path, max_pad_len=MAX_PAD_LEN):\n",
    "    y, sr = librosa.load(audio_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=26)\n",
    "    delta = librosa.feature.delta(mfcc)\n",
    "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    combined = np.concatenate((mfcc, delta, delta2), axis=0)\n",
    "    pad_width = max(0, max_pad_len - combined.shape[1])\n",
    "    combined = np.pad(combined, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    combined = combined.T[:max_pad_len]\n",
    "    return combined\n",
    "\n",
    "def extract_audio_from_video(video_path):\n",
    "    clip = VideoFileClip(video_path)\n",
    "    temp_audio = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)\n",
    "    clip.audio.write_audiofile(temp_audio.name, logger=None)\n",
    "    return temp_audio.name\n",
    "\n",
    "def predict_fusion_gradio(video_file):\n",
    "    audio_path = extract_audio_from_video(video_file)\n",
    "    audio_features = extract_audio_features(audio_path)\n",
    "    audio_features = audio_features[np.newaxis, ...]\n",
    "    audio_features_scaled = scaler.transform(audio_features.reshape(-1, audio_features.shape[2])).reshape(audio_features.shape)\n",
    "    audio_pred = audio_model.predict(audio_features_scaled)[0][0]\n",
    "    frames = extract_frames(video_file)\n",
    "    video_preds = video_model.predict(frames)\n",
    "    video_emotion_mean = np.mean(video_preds)\n",
    "    video_adjustment = VIDEO_INFLUENCE_WEIGHT * (video_emotion_mean - 0.5)\n",
    "    final_pred = audio_pred + video_adjustment\n",
    "    final_pred = np.clip(final_pred, 0, 1)\n",
    "    label = \"Depressed\" if final_pred >= 0.5 else \"Not Depressed\"\n",
    "    os.unlink(audio_path)\n",
    "    return f\"Label: {label}\"\n",
    "\n",
    "def launch_gradio():\n",
    "    gr.Interface(\n",
    "        fn=predict_fusion_gradio,\n",
    "        inputs=gr.File(file_types=[\".mp4\", \".flv\"], label=\"Upload MP4 or FLV Video\"),\n",
    "        outputs=\"text\",\n",
    "        title=\"Depression Detection\",\n",
    "        description=\"Upload a video (MP4/FLV) containing both audio and video. The model will predict depression status.\"\n",
    "    ).launch(share=True, debug=True)\n",
    "\n",
    "launch_gradio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5827,
     "status": "ok",
     "timestamp": 1750868540064,
     "user": {
      "displayName": "nouman Afzal",
      "userId": "16075913608289224866"
     },
     "user_tz": -300
    },
    "id": "RC6-BJ20y0fl",
    "outputId": "8b7a7c36-ff6e-4a6b-d802-91b6340dea22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\", force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 733
    },
    "executionInfo": {
     "elapsed": 239490,
     "status": "ok",
     "timestamp": 1750869802694,
     "user": {
      "displayName": "nouman Afzal",
      "userId": "16075913608289224866"
     },
     "user_tz": -300
    },
    "id": "njFBsSQM8dR7",
    "outputId": "3d530fdf-a393-4a2b-aa00-1cccfbc4e1d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/gradio/interface.py:416: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "* Running on public URL: https://246b4b9ceb5530241d.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://246b4b9ceb5530241d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 33 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7b2573300ea0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://246b4b9ceb5530241d.gradio.live\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import cv2\n",
    "import joblib\n",
    "import tempfile\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "import gradio as gr\n",
    "\n",
    "# Load models\n",
    "audio_model = tf.keras.models.load_model(\"/content/drive/MyDrive/Depression detection System (Audio + Video)/depression_model_finetuned (2).h5\")\n",
    "video_model = tf.keras.models.load_model(\"/content/drive/MyDrive/Depression Detection System (Video)/densenet201_depression_model.keras\")\n",
    "scaler = joblib.load(\"/content/drive/MyDrive/Depression detection System (Audio + Video)/scaler.pkl\")\n",
    "\n",
    "# Constants\n",
    "IMG_HEIGHT, IMG_WIDTH = 224, 224\n",
    "FRAMES_PER_VIDEO = 20\n",
    "MAX_PAD_LEN = 216\n",
    "VIDEO_INFLUENCE_WEIGHT = 0.4\n",
    "AUDIO_SR = 16000  # Fixed sample rate for audio consistency\n",
    "\n",
    "def extract_frames(video_path, num_frames=FRAMES_PER_VIDEO):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    if total_frames < num_frames:\n",
    "        frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    else:\n",
    "        start = max((total_frames - num_frames) // 2, 0)\n",
    "        frame_indices = range(start, start + num_frames)\n",
    "\n",
    "    frames = []\n",
    "    for fid in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, fid)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            frames.append(np.zeros((IMG_HEIGHT, IMG_WIDTH, 3), dtype=np.uint8))\n",
    "            continue\n",
    "        frame = cv2.resize(frame, (IMG_WIDTH, IMG_HEIGHT))\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "    frames = np.array(frames).astype('float32') / 255.0\n",
    "    return frames\n",
    "\n",
    "def extract_audio_features(audio_path, max_pad_len=MAX_PAD_LEN):\n",
    "    y, sr = librosa.load(audio_path, sr=AUDIO_SR)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=26)\n",
    "    delta = librosa.feature.delta(mfcc)\n",
    "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "    combined = np.concatenate((mfcc, delta, delta2), axis=0)\n",
    "    pad_width = max(0, max_pad_len - combined.shape[1])\n",
    "    combined = np.pad(combined, ((0, 0), (0, pad_width)), mode='constant')\n",
    "    combined = combined.T[:max_pad_len]\n",
    "    return combined\n",
    "\n",
    "def extract_audio_from_video(video_path):\n",
    "    clip = VideoFileClip(video_path)\n",
    "    temp_audio = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)\n",
    "    clip.audio.write_audiofile(temp_audio.name, fps=AUDIO_SR, logger=None)\n",
    "    return temp_audio.name\n",
    "\n",
    "def predict_fusion_gradio(video_file):\n",
    "    try:\n",
    "        audio_path = extract_audio_from_video(video_file)\n",
    "        audio_features = extract_audio_features(audio_path)\n",
    "        audio_features = audio_features[np.newaxis, ...]\n",
    "        audio_features_scaled = scaler.transform(\n",
    "            audio_features.reshape(-1, audio_features.shape[2])\n",
    "        ).reshape(audio_features.shape)\n",
    "\n",
    "        # Audio model prediction\n",
    "        audio_pred = audio_model.predict(audio_features_scaled, verbose=0)[0][0]\n",
    "\n",
    "        # Video model prediction\n",
    "        frames = extract_frames(video_file)\n",
    "        video_preds = video_model.predict(frames, verbose=0)\n",
    "        video_emotion_mean = np.mean(video_preds)\n",
    "        video_adjustment = VIDEO_INFLUENCE_WEIGHT * (video_emotion_mean - 0.5)\n",
    "\n",
    "        # Final fusion\n",
    "        final_pred = audio_pred + video_adjustment\n",
    "        final_pred = np.clip(final_pred, 0, 1)\n",
    "\n",
    "        # Label with buffer zone\n",
    "        if final_pred >= 0.55:\n",
    "            label = \"Depressed\"\n",
    "        elif final_pred <= 0.45:\n",
    "            label = \"Not Depressed\"\n",
    "        else:\n",
    "            label = \"Uncertain\"\n",
    "\n",
    "        os.unlink(audio_path)\n",
    "\n",
    "        # Log predictions\n",
    "        return (\n",
    "            f\"Label: {label}\\n\\n\"\n",
    "            f\"Audio Prediction: {audio_pred:.3f}\\n\"\n",
    "            f\"Video Adjustment: {video_adjustment:.3f}\\n\"\n",
    "            f\"Final Prediction Score: {final_pred:.3f}\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error during prediction: {str(e)}\"\n",
    "\n",
    "def launch_gradio():\n",
    "    gr.Interface(\n",
    "        fn=predict_fusion_gradio,\n",
    "        inputs=gr.File(file_types=[\".mp4\", \".flv\"], label=\"Upload MP4 or FLV Video\"),\n",
    "        outputs=\"text\",\n",
    "        title=\"Depression Detection System\",\n",
    "        description=\"Upload a video (MP4/FLV) with both audio and video. The model will classify as Depressed / Not Depressed / Uncertain.\",\n",
    "        allow_flagging=\"never\"\n",
    "    ).launch(share=True, debug=True)\n",
    "\n",
    "launch_gradio()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkFpk1T29SHe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
